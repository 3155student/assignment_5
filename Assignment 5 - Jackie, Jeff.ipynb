{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5 - Jackie Woodlief & Jeff Sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shelob/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's go ahead and set the random seed to one so we can get consistent results\n",
    "\n",
    "tf.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Picture\n",
    "\n",
    "Based on the past performance of the 2015 flight dataset (from assignment 3), we saw that the non-linear SVM's performed well on the data set; however, that was due in part because we reduced the size of our data set by quite a bit. If the Gaussian RBF with grid search can yield such good results on a small - medium size and complex data set, we can imagine that since neural networks are better at dealing with large, complex data sets, then the performance may even improve. The main reason being is that there will be more data. With such a big, complex data set, a neural network seems like the most appropriate approach.\n",
    "\n",
    "compare 2 different airlines, maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the Data / Info\n",
    "\n",
    "We are using the flight data set because it is our largest data set, it's massive, with almost 6,000,000 rows and 31 columns. In the past we couldn't use the entire data set because it was too big for our computers to handle efficiently. In addition, we already know that this is a complex data set due to our past assignments involving it and visualizing it.\n",
    "\n",
    "\n",
    "\n",
    "https://www.kaggle.com/usdot/flight-delays/data\n",
    "\n",
    "Click on the above link, and download flights.csv, as that is the data set I used for this assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shelob/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    " #Let's read in our data\n",
    "\n",
    "flights_data = pd.read_csv(\"flights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_throwaway, flights = train_test_split(flights_data, test_size = 0.17, random_state = 42)\n",
    "del train_set_throwaway # since we don't them, we can go ahead and delete them from the memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 989244 entries, 1508570 to 1367429\n",
      "Data columns (total 31 columns):\n",
      "YEAR                   989244 non-null int64\n",
      "MONTH                  989244 non-null int64\n",
      "DAY                    989244 non-null int64\n",
      "DAY_OF_WEEK            989244 non-null int64\n",
      "AIRLINE                989244 non-null object\n",
      "FLIGHT_NUMBER          989244 non-null int64\n",
      "TAIL_NUMBER            986799 non-null object\n",
      "ORIGIN_AIRPORT         989244 non-null object\n",
      "DESTINATION_AIRPORT    989244 non-null object\n",
      "SCHEDULED_DEPARTURE    989244 non-null int64\n",
      "DEPARTURE_TIME         974729 non-null float64\n",
      "DEPARTURE_DELAY        974729 non-null float64\n",
      "TAXI_OUT               974222 non-null float64\n",
      "WHEELS_OFF             974222 non-null float64\n",
      "SCHEDULED_TIME         989244 non-null float64\n",
      "ELAPSED_TIME           971487 non-null float64\n",
      "AIR_TIME               971487 non-null float64\n",
      "DISTANCE               989244 non-null int64\n",
      "WHEELS_ON              973616 non-null float64\n",
      "TAXI_IN                973616 non-null float64\n",
      "SCHEDULED_ARRIVAL      989244 non-null int64\n",
      "ARRIVAL_TIME           973616 non-null float64\n",
      "ARRIVAL_DELAY          971487 non-null float64\n",
      "DIVERTED               989244 non-null int64\n",
      "CANCELLED              989244 non-null int64\n",
      "CANCELLATION_REASON    15176 non-null object\n",
      "AIR_SYSTEM_DELAY       180586 non-null float64\n",
      "SECURITY_DELAY         180586 non-null float64\n",
      "AIRLINE_DELAY          180586 non-null float64\n",
      "LATE_AIRCRAFT_DELAY    180586 non-null float64\n",
      "WEATHER_DELAY          180586 non-null float64\n",
      "dtypes: float64(16), int64(10), object(5)\n",
      "memory usage: 241.5+ MB\n"
     ]
    }
   ],
   "source": [
    "flights.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>SCHEDULED_DEPARTURE</th>\n",
       "      <th>DEPARTURE_TIME</th>\n",
       "      <th>DEPARTURE_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>...</th>\n",
       "      <th>SCHEDULED_ARRIVAL</th>\n",
       "      <th>ARRIVAL_TIME</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>AIR_SYSTEM_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>AIRLINE_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>989244.0</td>\n",
       "      <td>989244.000000</td>\n",
       "      <td>989244.000000</td>\n",
       "      <td>989244.000000</td>\n",
       "      <td>989244.000000</td>\n",
       "      <td>989244.00000</td>\n",
       "      <td>974729.000000</td>\n",
       "      <td>974729.000000</td>\n",
       "      <td>974222.000000</td>\n",
       "      <td>974222.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>989244.000000</td>\n",
       "      <td>973616.000000</td>\n",
       "      <td>971487.000000</td>\n",
       "      <td>989244.000000</td>\n",
       "      <td>989244.000000</td>\n",
       "      <td>180586.000000</td>\n",
       "      <td>180586.000000</td>\n",
       "      <td>180586.000000</td>\n",
       "      <td>180586.000000</td>\n",
       "      <td>180586.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>6.521581</td>\n",
       "      <td>15.705901</td>\n",
       "      <td>3.924253</td>\n",
       "      <td>2171.461972</td>\n",
       "      <td>1329.54379</td>\n",
       "      <td>1335.244998</td>\n",
       "      <td>9.376492</td>\n",
       "      <td>16.075074</td>\n",
       "      <td>1357.214071</td>\n",
       "      <td>...</td>\n",
       "      <td>1493.999944</td>\n",
       "      <td>1476.876830</td>\n",
       "      <td>4.422846</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.015341</td>\n",
       "      <td>13.449121</td>\n",
       "      <td>0.074607</td>\n",
       "      <td>18.991876</td>\n",
       "      <td>23.515284</td>\n",
       "      <td>2.972323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.405019</td>\n",
       "      <td>8.783922</td>\n",
       "      <td>1.988866</td>\n",
       "      <td>1756.456337</td>\n",
       "      <td>483.62175</td>\n",
       "      <td>496.297124</td>\n",
       "      <td>37.207911</td>\n",
       "      <td>8.924512</td>\n",
       "      <td>497.906636</td>\n",
       "      <td>...</td>\n",
       "      <td>507.128619</td>\n",
       "      <td>526.259036</td>\n",
       "      <td>39.373835</td>\n",
       "      <td>0.051012</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>27.929466</td>\n",
       "      <td>1.945797</td>\n",
       "      <td>49.031444</td>\n",
       "      <td>42.955290</td>\n",
       "      <td>20.579862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-47.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-82.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>730.000000</td>\n",
       "      <td>917.00000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>935.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1110.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1689.000000</td>\n",
       "      <td>1325.00000</td>\n",
       "      <td>1330.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1343.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1520.000000</td>\n",
       "      <td>1513.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3225.000000</td>\n",
       "      <td>1730.00000</td>\n",
       "      <td>1739.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1754.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1918.000000</td>\n",
       "      <td>1917.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7438.000000</td>\n",
       "      <td>2359.00000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>1670.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2359.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>1665.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1101.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>1665.000000</td>\n",
       "      <td>1294.000000</td>\n",
       "      <td>937.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           YEAR          MONTH            DAY    DAY_OF_WEEK  FLIGHT_NUMBER  \\\n",
       "count  989244.0  989244.000000  989244.000000  989244.000000  989244.000000   \n",
       "mean     2015.0       6.521581      15.705901       3.924253    2171.461972   \n",
       "std         0.0       3.405019       8.783922       1.988866    1756.456337   \n",
       "min      2015.0       1.000000       1.000000       1.000000       1.000000   \n",
       "25%      2015.0       4.000000       8.000000       2.000000     730.000000   \n",
       "50%      2015.0       7.000000      16.000000       4.000000    1689.000000   \n",
       "75%      2015.0       9.000000      23.000000       6.000000    3225.000000   \n",
       "max      2015.0      12.000000      31.000000       7.000000    7438.000000   \n",
       "\n",
       "       SCHEDULED_DEPARTURE  DEPARTURE_TIME  DEPARTURE_DELAY       TAXI_OUT  \\\n",
       "count         989244.00000   974729.000000    974729.000000  974222.000000   \n",
       "mean            1329.54379     1335.244998         9.376492      16.075074   \n",
       "std              483.62175      496.297124        37.207911       8.924512   \n",
       "min                1.00000        1.000000       -47.000000       1.000000   \n",
       "25%              917.00000      921.000000        -5.000000      11.000000   \n",
       "50%             1325.00000     1330.000000        -2.000000      14.000000   \n",
       "75%             1730.00000     1739.000000         7.000000      19.000000   \n",
       "max             2359.00000     2400.000000      1670.000000     225.000000   \n",
       "\n",
       "          WHEELS_OFF      ...        SCHEDULED_ARRIVAL   ARRIVAL_TIME  \\\n",
       "count  974222.000000      ...            989244.000000  973616.000000   \n",
       "mean     1357.214071      ...              1493.999944    1476.876830   \n",
       "std       497.906636      ...               507.128619     526.259036   \n",
       "min         1.000000      ...                 1.000000       1.000000   \n",
       "25%       935.000000      ...              1110.000000    1059.000000   \n",
       "50%      1343.000000      ...              1520.000000    1513.000000   \n",
       "75%      1754.000000      ...              1918.000000    1917.000000   \n",
       "max      2400.000000      ...              2359.000000    2400.000000   \n",
       "\n",
       "       ARRIVAL_DELAY       DIVERTED      CANCELLED  AIR_SYSTEM_DELAY  \\\n",
       "count  971487.000000  989244.000000  989244.000000     180586.000000   \n",
       "mean        4.422846       0.002609       0.015341         13.449121   \n",
       "std        39.373835       0.051012       0.122905         27.929466   \n",
       "min       -82.000000       0.000000       0.000000          0.000000   \n",
       "25%       -13.000000       0.000000       0.000000          0.000000   \n",
       "50%        -5.000000       0.000000       0.000000          2.000000   \n",
       "75%         8.000000       0.000000       0.000000         18.000000   \n",
       "max      1665.000000       1.000000       1.000000       1101.000000   \n",
       "\n",
       "       SECURITY_DELAY  AIRLINE_DELAY  LATE_AIRCRAFT_DELAY  WEATHER_DELAY  \n",
       "count   180586.000000  180586.000000        180586.000000  180586.000000  \n",
       "mean         0.074607      18.991876            23.515284       2.972323  \n",
       "std          1.945797      49.031444            42.955290      20.579862  \n",
       "min          0.000000       0.000000             0.000000       0.000000  \n",
       "25%          0.000000       0.000000             0.000000       0.000000  \n",
       "50%          0.000000       2.000000             3.000000       0.000000  \n",
       "75%          0.000000      19.000000            29.000000       0.000000  \n",
       "max        221.000000    1665.000000          1294.000000     937.000000  \n",
       "\n",
       "[8 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>SCHEDULED_DEPARTURE</th>\n",
       "      <th>...</th>\n",
       "      <th>ARRIVAL_TIME</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_REASON</th>\n",
       "      <th>AIR_SYSTEM_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>AIRLINE_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1508570</th>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>EV</td>\n",
       "      <td>4900</td>\n",
       "      <td>N759EV</td>\n",
       "      <td>FWA</td>\n",
       "      <td>DTW</td>\n",
       "      <td>1340</td>\n",
       "      <td>...</td>\n",
       "      <td>1423.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363270</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>AS</td>\n",
       "      <td>611</td>\n",
       "      <td>N413AS</td>\n",
       "      <td>LAS</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1910</td>\n",
       "      <td>...</td>\n",
       "      <td>2133.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003945</th>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>WN</td>\n",
       "      <td>1483</td>\n",
       "      <td>N463WN</td>\n",
       "      <td>OAK</td>\n",
       "      <td>SEA</td>\n",
       "      <td>630</td>\n",
       "      <td>...</td>\n",
       "      <td>812.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291425</th>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>WN</td>\n",
       "      <td>193</td>\n",
       "      <td>N7745A</td>\n",
       "      <td>STL</td>\n",
       "      <td>DAL</td>\n",
       "      <td>810</td>\n",
       "      <td>...</td>\n",
       "      <td>1222.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2973084</th>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>UA</td>\n",
       "      <td>253</td>\n",
       "      <td>N213UA</td>\n",
       "      <td>IAH</td>\n",
       "      <td>HNL</td>\n",
       "      <td>1000</td>\n",
       "      <td>...</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         YEAR  MONTH  DAY  DAY_OF_WEEK AIRLINE  FLIGHT_NUMBER TAIL_NUMBER  \\\n",
       "1508570  2015      4    7            2      EV           4900      N759EV   \n",
       "363270   2015      1   24            6      AS            611      N413AS   \n",
       "3003945  2015      7    8            3      WN           1483      N463WN   \n",
       "2291425  2015      5   26            2      WN            193      N7745A   \n",
       "2973084  2015      7    6            1      UA            253      N213UA   \n",
       "\n",
       "        ORIGIN_AIRPORT DESTINATION_AIRPORT  SCHEDULED_DEPARTURE  \\\n",
       "1508570            FWA                 DTW                 1340   \n",
       "363270             LAS                 SEA                 1910   \n",
       "3003945            OAK                 SEA                  630   \n",
       "2291425            STL                 DAL                  810   \n",
       "2973084            IAH                 HNL                 1000   \n",
       "\n",
       "             ...        ARRIVAL_TIME  ARRIVAL_DELAY  DIVERTED  CANCELLED  \\\n",
       "1508570      ...              1423.0          -13.0         0          0   \n",
       "363270       ...              2133.0          -12.0         0          0   \n",
       "3003945      ...               812.0           -8.0         0          0   \n",
       "2291425      ...              1222.0          152.0         0          0   \n",
       "2973084      ...              1316.0           -2.0         0          0   \n",
       "\n",
       "         CANCELLATION_REASON  AIR_SYSTEM_DELAY  SECURITY_DELAY  AIRLINE_DELAY  \\\n",
       "1508570                  NaN               NaN             NaN            NaN   \n",
       "363270                   NaN               NaN             NaN            NaN   \n",
       "3003945                  NaN               NaN             NaN            NaN   \n",
       "2291425                  NaN               0.0             0.0            0.0   \n",
       "2973084                  NaN               NaN             NaN            NaN   \n",
       "\n",
       "         LATE_AIRCRAFT_DELAY  WEATHER_DELAY  \n",
       "1508570                  NaN            NaN  \n",
       "363270                   NaN            NaN  \n",
       "3003945                  NaN            NaN  \n",
       "2291425                152.0            0.0  \n",
       "2973084                  NaN            NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's make a copy just in case we need to refer back to the original\n",
    "\n",
    "copy = pd.DataFrame.copy(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on the correlation matrix of airlines, used from a previous run, we determined that these features\n",
    "# we not very correlated with determining the airline, so we decided to drop these features\n",
    "\n",
    "copy = copy.drop(\"CANCELLATION_REASON\", 1)\n",
    "copy = copy.drop(\"YEAR\", 1)\n",
    "copy = copy.drop(\"DAY\", 1)\n",
    "copy = copy.drop(\"MONTH\", 1)\n",
    "copy = copy.drop(\"DAY_OF_WEEK\", 1)\n",
    "copy = copy.drop(\"DIVERTED\", 1)\n",
    "copy = copy.drop(\"CANCELLED\", 1)\n",
    "copy = copy.drop(\"AIR_SYSTEM_DELAY\", 1)\n",
    "copy = copy.drop(\"SECURITY_DELAY\", 1)\n",
    "copy = copy.drop(\"WHEELS_OFF\", 1)\n",
    "copy = copy.drop(\"SCHEDULED_DEPARTURE\", 1)\n",
    "copy = copy.drop(\"WEATHER_DELAY\", 1)\n",
    "copy = copy.drop(\"AIRLINE_DELAY\", 1)\n",
    "copy = copy.drop(\"WHEELS_ON\", 1)\n",
    "copy = copy.drop(\"SCHEDULED_ARRIVAL\", 1)\n",
    "copy = copy.drop(\"ARRIVAL_TIME\", 1)\n",
    "copy = copy.drop(\"DEPARTURE_TIME\", 1)\n",
    "copy = copy.drop(\"LATE_AIRCRAFT_DELAY\", 1)\n",
    "copy = copy.drop(\"ARRIVAL_DELAY\", 1)\n",
    "copy = copy.drop(\"DEPARTURE_DELAY\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy = copy.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 989244 entries, 1508570 to 1367429\n",
      "Data columns (total 11 columns):\n",
      "AIRLINE                989244 non-null object\n",
      "FLIGHT_NUMBER          989244 non-null int64\n",
      "TAIL_NUMBER            986799 non-null object\n",
      "ORIGIN_AIRPORT         989244 non-null object\n",
      "DESTINATION_AIRPORT    989244 non-null object\n",
      "TAXI_OUT               974222 non-null float64\n",
      "SCHEDULED_TIME         989244 non-null float64\n",
      "ELAPSED_TIME           971487 non-null float64\n",
      "AIR_TIME               971487 non-null float64\n",
      "DISTANCE               989244 non-null int64\n",
      "TAXI_IN                973616 non-null float64\n",
      "dtypes: float64(5), int64(2), object(4)\n",
      "memory usage: 90.6+ MB\n"
     ]
    }
   ],
   "source": [
    "copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder() #AIRLINE, DESTINATION_AIRPORT, ORIGIN_AIRPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_cat = copy[\"AIRLINE\"]\n",
    "destination_cat = copy[\"DESTINATION_AIRPORT\"]\n",
    "origin_cat = copy[\"ORIGIN_AIRPORT\"]\n",
    "tail_cat = copy[\"TAIL_NUMBER\"]\n",
    "flight_cat = copy[\"FLIGHT_NUMBER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "destination_cat = destination_cat.apply(str)\n",
    "origin_cat = origin_cat.apply(str)\n",
    "flight_cat = flight_cat.apply(str)\n",
    "tail_cat = tail_cat.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# destination_cat = destination_cat.fillna('Other Dest')\n",
    "# origin_cat = origin_cat.fillna('Other Org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_cat_encoded = encoder.fit_transform(airline_cat)\n",
    "destination_cat_encoded = encoder.fit_transform(destination_cat)\n",
    "origin_cat_encoded = encoder.fit_transform(origin_cat)\n",
    "tail_cat_encoded = encoder.fit_transform(tail_cat)\n",
    "flight_cat_encoded = encoder.fit_transform(flight_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copy[\"AIRLINE\"]= airline_cat_encoded\n",
    "copy[\"DESTINATION_AIRPORT\"] = destination_cat_encoded\n",
    "copy[\"ORIGIN_AIRPORT\"] = origin_cat_encoded\n",
    "copy[\"FLIGHT_NUMBER\"] = flight_cat_encoded\n",
    "copy[\"TAIL_NUMBER\"] = tail_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>TAXI_IN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1508570</th>\n",
       "      <td>4</td>\n",
       "      <td>4333</td>\n",
       "      <td>3386</td>\n",
       "      <td>422</td>\n",
       "      <td>396</td>\n",
       "      <td>13.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>128</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363270</th>\n",
       "      <td>1</td>\n",
       "      <td>5669</td>\n",
       "      <td>1627</td>\n",
       "      <td>478</td>\n",
       "      <td>580</td>\n",
       "      <td>14.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>867</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003945</th>\n",
       "      <td>13</td>\n",
       "      <td>537</td>\n",
       "      <td>1788</td>\n",
       "      <td>526</td>\n",
       "      <td>580</td>\n",
       "      <td>8.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>672</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         AIRLINE  FLIGHT_NUMBER  TAIL_NUMBER  ORIGIN_AIRPORT  \\\n",
       "1508570        4           4333         3386             422   \n",
       "363270         1           5669         1627             478   \n",
       "3003945       13            537         1788             526   \n",
       "\n",
       "         DESTINATION_AIRPORT  TAXI_OUT  SCHEDULED_TIME  ELAPSED_TIME  \\\n",
       "1508570                  396      13.0            56.0          48.0   \n",
       "363270                   580      14.0           155.0         155.0   \n",
       "3003945                  580       8.0           110.0         106.0   \n",
       "\n",
       "         AIR_TIME  DISTANCE  TAXI_IN  \n",
       "1508570      30.0       128      5.0  \n",
       "363270      131.0       867     10.0  \n",
       "3003945      92.0       672      6.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy = \"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>TAXI_IN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4333.0</td>\n",
       "      <td>3386.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5669.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>1788.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>672.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1033.0</td>\n",
       "      <td>3525.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>3904.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>4787.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6157.0</td>\n",
       "      <td>2090.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1107.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3667.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6383.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1607.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>4148.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1398.0</td>\n",
       "      <td>4118.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>588.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>3539.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>3294.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>612.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>2249.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1749.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>922.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6563.0</td>\n",
       "      <td>911.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>519.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>4078.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1475.0</td>\n",
       "      <td>1834.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>853.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3654.0</td>\n",
       "      <td>4451.0</td>\n",
       "      <td>542.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>2370.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1044.0</td>\n",
       "      <td>1921.0</td>\n",
       "      <td>445.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>2603.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4431.0</td>\n",
       "      <td>3628.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3987.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4050.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2524.0</td>\n",
       "      <td>4619.0</td>\n",
       "      <td>506.0</td>\n",
       "      <td>606.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>1204.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>1487.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>987.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3906.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2133.0</td>\n",
       "      <td>3375.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989214</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3788.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>365.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989215</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3499.0</td>\n",
       "      <td>2548.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>456.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989216</th>\n",
       "      <td>3.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989217</th>\n",
       "      <td>13.0</td>\n",
       "      <td>6542.0</td>\n",
       "      <td>4319.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989218</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2608.0</td>\n",
       "      <td>3801.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989219</th>\n",
       "      <td>5.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>581.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989220</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2490.0</td>\n",
       "      <td>3929.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989221</th>\n",
       "      <td>13.0</td>\n",
       "      <td>5636.0</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989222</th>\n",
       "      <td>13.0</td>\n",
       "      <td>3663.0</td>\n",
       "      <td>4137.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989223</th>\n",
       "      <td>10.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>409.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1372.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989224</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>2670.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989225</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6721.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989226</th>\n",
       "      <td>13.0</td>\n",
       "      <td>3909.0</td>\n",
       "      <td>2265.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>794.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989227</th>\n",
       "      <td>3.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>4673.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989228</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6597.0</td>\n",
       "      <td>4666.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989229</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6396.0</td>\n",
       "      <td>2105.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989230</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4633.0</td>\n",
       "      <td>4456.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989231</th>\n",
       "      <td>0.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1391.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989232</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5180.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989233</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2758.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>589.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989234</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>388.0</td>\n",
       "      <td>507.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>634.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989235</th>\n",
       "      <td>11.0</td>\n",
       "      <td>3888.0</td>\n",
       "      <td>3018.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989236</th>\n",
       "      <td>13.0</td>\n",
       "      <td>2678.0</td>\n",
       "      <td>712.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989237</th>\n",
       "      <td>3.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>4789.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989238</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4196.0</td>\n",
       "      <td>2556.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989239</th>\n",
       "      <td>3.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989240</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2909.0</td>\n",
       "      <td>2181.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>488.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989241</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>1625.0</td>\n",
       "      <td>389.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989242</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2877.0</td>\n",
       "      <td>2091.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>2465.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989243</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1453.0</td>\n",
       "      <td>4594.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>989244 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AIRLINE  FLIGHT_NUMBER  TAIL_NUMBER  ORIGIN_AIRPORT  \\\n",
       "0           4.0         4333.0       3386.0           422.0   \n",
       "1           1.0         5669.0       1627.0           478.0   \n",
       "2          13.0          537.0       1788.0           526.0   \n",
       "3          13.0         1033.0       3525.0           598.0   \n",
       "4          10.0         1700.0        582.0           455.0   \n",
       "5           3.0          424.0       4787.0           450.0   \n",
       "6          10.0          510.0        714.0           455.0   \n",
       "7           1.0         6157.0       2090.0           581.0   \n",
       "8           0.0            0.0       3667.0           470.0   \n",
       "9           5.0         6383.0        567.0           251.0   \n",
       "10         13.0         1682.0       4148.0           506.0   \n",
       "11         13.0         1398.0       4118.0           497.0   \n",
       "12         13.0         2611.0       3539.0           478.0   \n",
       "13         13.0          622.0       3294.0           607.0   \n",
       "14         11.0         1091.0       2249.0           390.0   \n",
       "15          4.0         1749.0       3000.0           390.0   \n",
       "16          3.0         6563.0        911.0           343.0   \n",
       "17          0.0          270.0       4078.0           497.0   \n",
       "18          0.0         1475.0       1834.0           508.0   \n",
       "19         11.0         3654.0       4451.0           542.0   \n",
       "20          6.0         1044.0       1921.0           445.0   \n",
       "21         13.0         1298.0       2603.0           354.0   \n",
       "22          1.0         4431.0       3628.0           572.0   \n",
       "23          0.0         3987.0        502.0           271.0   \n",
       "24          4.0         4050.0        161.0           410.0   \n",
       "25         13.0         2524.0       4619.0           506.0   \n",
       "26          0.0          396.0       1487.0           326.0   \n",
       "27          2.0          286.0        987.0           453.0   \n",
       "28          4.0         3906.0        254.0           455.0   \n",
       "29         13.0         2133.0       3375.0           461.0   \n",
       "...         ...            ...          ...             ...   \n",
       "989214     10.0         3788.0       2008.0           470.0   \n",
       "989215      8.0         3499.0       2548.0           482.0   \n",
       "989216      3.0          176.0        873.0           330.0   \n",
       "989217     13.0         6542.0       4319.0            47.0   \n",
       "989218      7.0         2608.0       3801.0           340.0   \n",
       "989219      5.0          467.0        546.0           581.0   \n",
       "989220      7.0         2490.0       3929.0           544.0   \n",
       "989221     13.0         5636.0       1314.0           324.0   \n",
       "989222     13.0         3663.0       4137.0           531.0   \n",
       "989223     10.0          131.0        128.0           390.0   \n",
       "989224      2.0         2300.0       2670.0           557.0   \n",
       "989225      3.0         6721.0       1078.0           590.0   \n",
       "989226     13.0         3909.0       2265.0           389.0   \n",
       "989227      3.0          580.0       4673.0           324.0   \n",
       "989228      3.0         6597.0       4666.0           607.0   \n",
       "989229      1.0         6396.0       2105.0           590.0   \n",
       "989230      4.0         4633.0       4456.0           324.0   \n",
       "989231      0.0          250.0       1486.0           470.0   \n",
       "989232      4.0         5180.0        640.0           455.0   \n",
       "989233      9.0         2758.0        600.0           540.0   \n",
       "989234     13.0         2160.0       1606.0           388.0   \n",
       "989235     11.0         3888.0       3018.0           370.0   \n",
       "989236     13.0         2678.0        712.0           326.0   \n",
       "989237      3.0          311.0       4789.0           324.0   \n",
       "989238      9.0         4196.0       2556.0           425.0   \n",
       "989239      3.0          566.0        902.0           590.0   \n",
       "989240      7.0         2909.0       2181.0           532.0   \n",
       "989241     10.0         3122.0       1625.0           389.0   \n",
       "989242      2.0         2877.0       2091.0           470.0   \n",
       "989243      3.0         1453.0       4594.0           324.0   \n",
       "\n",
       "        DESTINATION_AIRPORT  TAXI_OUT  SCHEDULED_TIME  ELAPSED_TIME  AIR_TIME  \\\n",
       "0                     396.0      13.0            56.0          48.0      30.0   \n",
       "1                     580.0      14.0           155.0         155.0     131.0   \n",
       "2                     580.0       8.0           110.0         106.0      92.0   \n",
       "3                     384.0       6.0           100.0          99.0      85.0   \n",
       "4                     444.0      16.0           498.0         488.0     461.0   \n",
       "5                     323.0      13.0            70.0          49.0      28.0   \n",
       "6                     606.0      13.0           137.0         120.0     100.0   \n",
       "7                     542.0      11.0           171.0         163.0     145.0   \n",
       "8                     479.0      34.0           385.0         381.0     341.0   \n",
       "9                      80.0       9.0           249.0         222.0     204.0   \n",
       "10                    354.0      13.0           115.0          92.0      72.0   \n",
       "11                    588.0       8.0           175.0         157.0     145.0   \n",
       "12                    590.0      17.0            95.0          87.0      64.0   \n",
       "13                    340.0      21.0           105.0         104.0      78.0   \n",
       "14                    369.0      15.0           143.0         146.0     116.0   \n",
       "15                    356.0      18.0           131.0         129.0     107.0   \n",
       "16                    519.0      31.0           205.0         196.0     161.0   \n",
       "17                    389.0      14.0           183.0         174.0     148.0   \n",
       "18                    389.0      10.0           175.0         139.0     118.0   \n",
       "19                    571.0      10.0           340.0         315.0     302.0   \n",
       "20                    526.0       9.0            39.0          36.0      21.0   \n",
       "21                    525.0      13.0            70.0          85.0      67.0   \n",
       "22                    580.0      19.0           180.0         186.0     161.0   \n",
       "23                    225.0      21.0            95.0         100.0      74.0   \n",
       "24                    387.0      44.0            80.0          98.0      49.0   \n",
       "25                    606.0       8.0           220.0         191.0     178.0   \n",
       "26                    389.0      14.0            65.0         118.0      94.0   \n",
       "27                    342.0      31.0            80.0          97.0      61.0   \n",
       "28                    448.0      19.0            70.0          68.0      46.0   \n",
       "29                    495.0      11.0            95.0          96.0      80.0   \n",
       "...                     ...       ...             ...           ...       ...   \n",
       "989214                479.0      18.0           365.0         354.0     329.0   \n",
       "989215                523.0      12.0            91.0          91.0      71.0   \n",
       "989216                396.0      13.0           124.0         115.0      96.0   \n",
       "989217                208.0      10.0            70.0          68.0      52.0   \n",
       "989218                531.0       7.0            58.0          46.0      30.0   \n",
       "989219                388.0      18.0           150.0         154.0     124.0   \n",
       "989220                531.0       9.0            60.0          48.0      27.0   \n",
       "989221                546.0      23.0           110.0         107.0      78.0   \n",
       "989222                525.0      10.0            90.0          72.0      54.0   \n",
       "989223                409.0      14.0           198.0         201.0     177.0   \n",
       "989224                469.0      10.0            86.0          61.0      48.0   \n",
       "989225                531.0      14.0           189.0         192.0     163.0   \n",
       "989226                572.0       8.0           120.0         114.0     103.0   \n",
       "989227                385.0      17.0            86.0          89.0      66.0   \n",
       "989228                323.0      14.0            94.0          85.0      62.0   \n",
       "989229                580.0      10.0           120.0         118.0     101.0   \n",
       "989230                607.0      18.0            66.0          66.0      43.0   \n",
       "989231                389.0      24.0           244.0         233.0     193.0   \n",
       "989232                350.0      26.0            63.0          77.0      47.0   \n",
       "989233                589.0      10.0           106.0          96.0      80.0   \n",
       "989234                507.0       9.0           125.0         105.0      91.0   \n",
       "989235                323.0      14.0            76.0          61.0      41.0   \n",
       "989236                525.0      14.0           225.0         213.0     193.0   \n",
       "989237                384.0      18.0           138.0         128.0     103.0   \n",
       "989238                580.0      21.0            66.0          67.0      42.0   \n",
       "989239                495.0      24.0           144.0         146.0     115.0   \n",
       "989240                488.0      14.0           106.0          98.0      79.0   \n",
       "989241                546.0      12.0           176.0         166.0     150.0   \n",
       "989242                353.0      25.0           380.0         399.0     369.0   \n",
       "989243                573.0      20.0            69.0          57.0      33.0   \n",
       "\n",
       "        DISTANCE  TAXI_IN  \n",
       "0          128.0      5.0  \n",
       "1          867.0     10.0  \n",
       "2          672.0      6.0  \n",
       "3          546.0      8.0  \n",
       "4         3904.0     11.0  \n",
       "5          151.0      8.0  \n",
       "6          787.0      7.0  \n",
       "7         1107.0      7.0  \n",
       "8         2475.0      6.0  \n",
       "9         1607.0      9.0  \n",
       "10         377.0      7.0  \n",
       "11        1189.0      4.0  \n",
       "12         397.0      6.0  \n",
       "13         612.0      5.0  \n",
       "14         936.0     15.0  \n",
       "15         922.0      4.0  \n",
       "16        1124.0      4.0  \n",
       "17         985.0     12.0  \n",
       "18         853.0     11.0  \n",
       "19        2370.0      3.0  \n",
       "20         100.0      6.0  \n",
       "21         325.0      5.0  \n",
       "22        1050.0      6.0  \n",
       "23         507.0      5.0  \n",
       "24         199.0      5.0  \n",
       "25        1204.0      5.0  \n",
       "26         190.0      6.0  \n",
       "27         413.0      5.0  \n",
       "28         295.0      3.0  \n",
       "29         451.0      5.0  \n",
       "...          ...      ...  \n",
       "989214    2475.0      7.0  \n",
       "989215     456.0      8.0  \n",
       "989216     549.0      6.0  \n",
       "989217     325.0      6.0  \n",
       "989218     116.0      9.0  \n",
       "989219    1024.0     12.0  \n",
       "989220     130.0     12.0  \n",
       "989221     526.0      6.0  \n",
       "989222     362.0      8.0  \n",
       "989223    1372.0     10.0  \n",
       "989224     273.0      3.0  \n",
       "989225    1250.0     15.0  \n",
       "989226     794.0      3.0  \n",
       "989227     432.0      6.0  \n",
       "989228     406.0      9.0  \n",
       "989229     689.0      7.0  \n",
       "989230     227.0      5.0  \n",
       "989231    1391.0     16.0  \n",
       "989232     253.0      4.0  \n",
       "989233     630.0      6.0  \n",
       "989234     634.0      5.0  \n",
       "989235     226.0      6.0  \n",
       "989236    1497.0      6.0  \n",
       "989237     721.0      7.0  \n",
       "989238     224.0      4.0  \n",
       "989239     920.0      7.0  \n",
       "989240     552.0      5.0  \n",
       "989241    1290.0      4.0  \n",
       "989242    2465.0      5.0  \n",
       "989243     214.0      4.0  \n",
       "\n",
       "[989244 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(copy)\n",
    "X = imputer.transform(copy)\n",
    "transform_copy = pd.DataFrame(X, columns = copy.columns)\n",
    "transform_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ8AAANeCAYAAAC8nq/uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2b8fd332b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "transform_copy.hist(bins = 50, figsize = (20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = transform_copy.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIRLINE                1.000000\n",
       "FLIGHT_NUMBER          0.111047\n",
       "TAIL_NUMBER            0.095959\n",
       "ORIGIN_AIRPORT         0.076239\n",
       "DESTINATION_AIRPORT    0.074975\n",
       "TAXI_IN               -0.079668\n",
       "DISTANCE              -0.082375\n",
       "AIR_TIME              -0.086422\n",
       "SCHEDULED_TIME        -0.105774\n",
       "ELAPSED_TIME          -0.110566\n",
       "TAXI_OUT              -0.170568\n",
       "Name: AIRLINE, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix[\"AIRLINE\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(transform_copy, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>TAXI_IN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>882994</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6402.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249778</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5083.0</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30609</th>\n",
       "      <td>12.0</td>\n",
       "      <td>6565.0</td>\n",
       "      <td>4007.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>539.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AIRLINE  FLIGHT_NUMBER  TAIL_NUMBER  ORIGIN_AIRPORT  \\\n",
       "882994      3.0         6402.0       1128.0           480.0   \n",
       "249778      9.0         5083.0       3688.0           326.0   \n",
       "30609      12.0         6565.0       4007.0           582.0   \n",
       "\n",
       "        DESTINATION_AIRPORT  TAXI_OUT  SCHEDULED_TIME  ELAPSED_TIME  AIR_TIME  \\\n",
       "882994                520.0      15.0           221.0         217.0     198.0   \n",
       "249778                454.0      15.0            52.0          55.0      34.0   \n",
       "30609                 539.0      11.0           100.0          94.0      81.0   \n",
       "\n",
       "        DISTANCE  TAXI_IN  \n",
       "882994    1670.0      4.0  \n",
       "249778     140.0      6.0  \n",
       "30609      550.0      2.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>TAXI_IN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>686489</th>\n",
       "      <td>13.0</td>\n",
       "      <td>3910.0</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>495.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585468</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2689.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>1846.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705282</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4618.0</td>\n",
       "      <td>3148.0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1428.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        AIRLINE  FLIGHT_NUMBER  TAIL_NUMBER  ORIGIN_AIRPORT  \\\n",
       "686489     13.0         3910.0       1668.0           526.0   \n",
       "585468      0.0         2689.0       1390.0           263.0   \n",
       "705282      9.0         4618.0       3148.0           455.0   \n",
       "\n",
       "        DESTINATION_AIRPORT  TAXI_OUT  SCHEDULED_TIME  ELAPSED_TIME  AIR_TIME  \\\n",
       "686489                495.0       8.0           200.0         206.0     193.0   \n",
       "585468                214.0      17.0           255.0         245.0     210.0   \n",
       "705282                331.0      16.0           227.0         208.0     187.0   \n",
       "\n",
       "        DISTANCE  TAXI_IN  \n",
       "686489    1489.0      5.0  \n",
       "585468    1846.0     18.0  \n",
       "705282    1428.0      5.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we need to split up into test and train sets based on the airline (multi-class)\n",
    "\n",
    "y_train = train_set[\"AIRLINE\"]\n",
    "y_test = test_set[\"AIRLINE\"]\n",
    "x_test = test_set.drop(\"AIRLINE\", 1)\n",
    "x_train = train_set.drop(\"AIRLINE\", 1)\n",
    "\n",
    "\n",
    "# We need to convert to float32 due to a warning we get when we do the dnn_clf\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "882994    3.0\n",
       "249778    9.0\n",
       "Name: AIRLINE, dtype: float32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>SCHEDULED_TIME</th>\n",
       "      <th>ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>TAXI_IN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>882994</th>\n",
       "      <td>6402.0</td>\n",
       "      <td>1128.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249778</th>\n",
       "      <td>5083.0</td>\n",
       "      <td>3688.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FLIGHT_NUMBER  TAIL_NUMBER  ORIGIN_AIRPORT  DESTINATION_AIRPORT  \\\n",
       "882994         6402.0       1128.0           480.0                520.0   \n",
       "249778         5083.0       3688.0           326.0                454.0   \n",
       "\n",
       "        TAXI_OUT  SCHEDULED_TIME  ELAPSED_TIME  AIR_TIME  DISTANCE  TAXI_IN  \n",
       "882994      15.0           221.0         217.0     198.0    1670.0      4.0  \n",
       "249778      15.0            52.0          55.0      34.0     140.0      6.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Visualization\n",
    "\n",
    "Below we will be visualizing different aspects of our data to demonstrate that it is in fact a complex data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before we do anything, in order to feed the various clf's our labels, we must first convert them from\n",
    "# float to integers\n",
    "\n",
    "y_train_int = y_train.astype(int)\n",
    "y_test_int = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_train_ds = x_train.values\n",
    "# x_test_ds = x_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction and Execution Phases\n",
    "\n",
    "Using the built-in DNNClassifier(), the construction and execution phases are handled already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully connected network with 2 hidden layers\n",
    "\n",
    "This is the neural network that we start with!\n",
    "\n",
    "Also, just to note, since we don't specify here, the activation function being used is the relu (default)!\n",
    "\n",
    "### Observations\n",
    "\n",
    "First, we wanted to point out that we have the step size set to 50,000; however, the higher it is, the better performance it seems to get. We tested it at 100,000 and saw ~58% accuracy instead of ~53% at 50,000 steps (once at 50,000 steps we saw ~55% accuracy). We didn't test above 100,000 steps. The reason that we left it at 50,000 is so that it will execute faster. so 53-58% may not sound very accurate, but when you consider that we have 14 different airlines and that if you predicted only one of the fourteen for the test set, you would expect to see around a 7.14% accuracy (it depends on the distribution, this assumes that every airline is equally represented in the data set.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791395, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpdu3z1poh\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2b8e446e80>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmpdu3z1poh'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpdu3z1poh/model.ckpt.\n",
      "INFO:tensorflow:loss = 338.58734, step = 1\n",
      "INFO:tensorflow:global_step/sec: 84.1987\n",
      "INFO:tensorflow:loss = 2.2785523, step = 101 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5981\n",
      "INFO:tensorflow:loss = 2.4340935, step = 201 (1.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0189\n",
      "INFO:tensorflow:loss = 1.883375, step = 301 (1.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.8324\n",
      "INFO:tensorflow:loss = 2.3161337, step = 401 (1.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.8445\n",
      "INFO:tensorflow:loss = 2.1711562, step = 501 (1.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4634\n",
      "INFO:tensorflow:loss = 2.3671525, step = 601 (1.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0039\n",
      "INFO:tensorflow:loss = 1.905809, step = 701 (1.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.5027\n",
      "INFO:tensorflow:loss = 1.996328, step = 801 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5494\n",
      "INFO:tensorflow:loss = 2.073579, step = 901 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.6909\n",
      "INFO:tensorflow:loss = 1.9367274, step = 1001 (1.113 sec)\n",
      "INFO:tensorflow:global_step/sec: 93.387\n",
      "INFO:tensorflow:loss = 1.866434, step = 1101 (1.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.0965\n",
      "INFO:tensorflow:loss = 2.1000745, step = 1201 (1.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.3554\n",
      "INFO:tensorflow:loss = 1.804664, step = 1301 (1.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3937\n",
      "INFO:tensorflow:loss = 2.0171177, step = 1401 (1.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2052\n",
      "INFO:tensorflow:loss = 1.8014265, step = 1501 (1.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7357\n",
      "INFO:tensorflow:loss = 1.823535, step = 1601 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.159\n",
      "INFO:tensorflow:loss = 1.9565274, step = 1701 (1.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.9029\n",
      "INFO:tensorflow:loss = 1.4788258, step = 1801 (1.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3303\n",
      "INFO:tensorflow:loss = 1.84952, step = 1901 (1.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9287\n",
      "INFO:tensorflow:loss = 1.7623879, step = 2001 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9983\n",
      "INFO:tensorflow:loss = 1.9862305, step = 2101 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.218\n",
      "INFO:tensorflow:loss = 1.8767035, step = 2201 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2597\n",
      "INFO:tensorflow:loss = 1.3471274, step = 2301 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1651\n",
      "INFO:tensorflow:loss = 1.7206639, step = 2401 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4854\n",
      "INFO:tensorflow:loss = 1.6872067, step = 2501 (1.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0203\n",
      "INFO:tensorflow:loss = 1.4647292, step = 2601 (1.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6035\n",
      "INFO:tensorflow:loss = 1.7316589, step = 2701 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0295\n",
      "INFO:tensorflow:loss = 1.8493382, step = 2801 (1.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2477\n",
      "INFO:tensorflow:loss = 2.2841735, step = 2901 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8162\n",
      "INFO:tensorflow:loss = 1.686736, step = 3001 (1.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6524\n",
      "INFO:tensorflow:loss = 1.9517022, step = 3101 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.0236\n",
      "INFO:tensorflow:loss = 1.5556018, step = 3201 (1.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5641\n",
      "INFO:tensorflow:loss = 1.4786842, step = 3301 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8084\n",
      "INFO:tensorflow:loss = 1.7081223, step = 3401 (1.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.9176\n",
      "INFO:tensorflow:loss = 1.8322307, step = 3501 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.8689\n",
      "INFO:tensorflow:loss = 1.430849, step = 3601 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1548\n",
      "INFO:tensorflow:loss = 1.9292691, step = 3701 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.8185\n",
      "INFO:tensorflow:loss = 1.7035246, step = 3801 (1.433 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.7576\n",
      "INFO:tensorflow:loss = 1.5882597, step = 3901 (1.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8624\n",
      "INFO:tensorflow:loss = 1.6318306, step = 4001 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.7816\n",
      "INFO:tensorflow:loss = 1.6109439, step = 4101 (1.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1905\n",
      "INFO:tensorflow:loss = 1.8750539, step = 4201 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4305\n",
      "INFO:tensorflow:loss = 1.4272157, step = 4301 (1.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0588\n",
      "INFO:tensorflow:loss = 1.4460179, step = 4401 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.4741\n",
      "INFO:tensorflow:loss = 1.5240569, step = 4501 (1.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.079\n",
      "INFO:tensorflow:loss = 1.6944718, step = 4601 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.3358\n",
      "INFO:tensorflow:loss = 1.6725239, step = 4701 (1.401 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.9619\n",
      "INFO:tensorflow:loss = 1.890239, step = 4801 (1.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3377\n",
      "INFO:tensorflow:loss = 1.8828601, step = 4901 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0451\n",
      "INFO:tensorflow:loss = 1.2899417, step = 5001 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.0804\n",
      "INFO:tensorflow:loss = 1.646236, step = 5101 (1.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.7526\n",
      "INFO:tensorflow:loss = 1.3534573, step = 5201 (1.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3437\n",
      "INFO:tensorflow:loss = 1.4974413, step = 5301 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.1929\n",
      "INFO:tensorflow:loss = 1.4281813, step = 5401 (1.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.5289\n",
      "INFO:tensorflow:loss = 1.354235, step = 5501 (1.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.686\n",
      "INFO:tensorflow:loss = 1.4305778, step = 5601 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.8033\n",
      "INFO:tensorflow:loss = 1.3564454, step = 5701 (1.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4063\n",
      "INFO:tensorflow:loss = 1.2483546, step = 5801 (1.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0505\n",
      "INFO:tensorflow:loss = 1.4481486, step = 5901 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4575\n",
      "INFO:tensorflow:loss = 1.4877098, step = 6001 (1.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7779\n",
      "INFO:tensorflow:loss = 1.9180014, step = 6101 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3484\n",
      "INFO:tensorflow:loss = 1.8124604, step = 6201 (1.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1649\n",
      "INFO:tensorflow:loss = 1.5755609, step = 6301 (1.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8666\n",
      "INFO:tensorflow:loss = 1.6299922, step = 6401 (1.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.7133\n",
      "INFO:tensorflow:loss = 1.4284661, step = 6501 (1.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.2207\n",
      "INFO:tensorflow:loss = 1.5627409, step = 6601 (1.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.7696\n",
      "INFO:tensorflow:loss = 1.4013159, step = 6701 (1.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.6984\n",
      "INFO:tensorflow:loss = 1.4724976, step = 6801 (1.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.5544\n",
      "INFO:tensorflow:loss = 1.7458194, step = 6901 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0479\n",
      "INFO:tensorflow:loss = 1.6749663, step = 7001 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.9796\n",
      "INFO:tensorflow:loss = 1.4398718, step = 7101 (1.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0868\n",
      "INFO:tensorflow:loss = 1.5315977, step = 7201 (1.122 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 87.1422\n",
      "INFO:tensorflow:loss = 1.7104034, step = 7301 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.4105\n",
      "INFO:tensorflow:loss = 1.203406, step = 7401 (1.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.029\n",
      "INFO:tensorflow:loss = 1.4716808, step = 7501 (1.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.6267\n",
      "INFO:tensorflow:loss = 1.6916428, step = 7601 (1.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0659\n",
      "INFO:tensorflow:loss = 1.5816171, step = 7701 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.4407\n",
      "INFO:tensorflow:loss = 1.58824, step = 7801 (1.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.557\n",
      "INFO:tensorflow:loss = 1.8060086, step = 7901 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.936\n",
      "INFO:tensorflow:loss = 1.746879, step = 8001 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.2542\n",
      "INFO:tensorflow:loss = 1.6606097, step = 8101 (1.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5186\n",
      "INFO:tensorflow:loss = 1.7601298, step = 8201 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.9721\n",
      "INFO:tensorflow:loss = 1.3775498, step = 8301 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.3917\n",
      "INFO:tensorflow:loss = 1.5869716, step = 8401 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.791\n",
      "INFO:tensorflow:loss = 1.4957936, step = 8501 (1.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.1441\n",
      "INFO:tensorflow:loss = 1.3797516, step = 8601 (1.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.4586\n",
      "INFO:tensorflow:loss = 1.351424, step = 8701 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.9737\n",
      "INFO:tensorflow:loss = 1.4745423, step = 8801 (1.111 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4632\n",
      "INFO:tensorflow:loss = 1.5064272, step = 8901 (1.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0102\n",
      "INFO:tensorflow:loss = 1.6255194, step = 9001 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5411\n",
      "INFO:tensorflow:loss = 1.4770081, step = 9101 (1.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.282\n",
      "INFO:tensorflow:loss = 1.642311, step = 9201 (1.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.415\n",
      "INFO:tensorflow:loss = 1.4613605, step = 9301 (1.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.1006\n",
      "INFO:tensorflow:loss = 1.4298974, step = 9401 (1.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9614\n",
      "INFO:tensorflow:loss = 1.5160216, step = 9501 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.6405\n",
      "INFO:tensorflow:loss = 1.5772213, step = 9601 (1.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.7256\n",
      "INFO:tensorflow:loss = 1.4503353, step = 9701 (1.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.555\n",
      "INFO:tensorflow:loss = 1.6307095, step = 9801 (1.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.9164\n",
      "INFO:tensorflow:loss = 1.5456315, step = 9901 (1.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4053\n",
      "INFO:tensorflow:loss = 1.3674626, step = 10001 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9458\n",
      "INFO:tensorflow:loss = 1.622503, step = 10101 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8319\n",
      "INFO:tensorflow:loss = 1.5721043, step = 10201 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0815\n",
      "INFO:tensorflow:loss = 1.7223865, step = 10301 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6358\n",
      "INFO:tensorflow:loss = 1.5235791, step = 10401 (1.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7095\n",
      "INFO:tensorflow:loss = 1.8418082, step = 10501 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2598\n",
      "INFO:tensorflow:loss = 1.5090626, step = 10601 (1.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2565\n",
      "INFO:tensorflow:loss = 1.7155153, step = 10701 (1.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.65\n",
      "INFO:tensorflow:loss = 1.5785301, step = 10801 (1.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4292\n",
      "INFO:tensorflow:loss = 1.2696766, step = 10901 (1.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5855\n",
      "INFO:tensorflow:loss = 1.3253961, step = 11001 (1.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.9938\n",
      "INFO:tensorflow:loss = 1.4790626, step = 11101 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.1311\n",
      "INFO:tensorflow:loss = 1.4324698, step = 11201 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1876\n",
      "INFO:tensorflow:loss = 1.4662842, step = 11301 (1.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3645\n",
      "INFO:tensorflow:loss = 1.5002385, step = 11401 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6964\n",
      "INFO:tensorflow:loss = 1.56669, step = 11501 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5681\n",
      "INFO:tensorflow:loss = 1.6050959, step = 11601 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5981\n",
      "INFO:tensorflow:loss = 1.4060073, step = 11701 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5209\n",
      "INFO:tensorflow:loss = 1.5148301, step = 11801 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.3737\n",
      "INFO:tensorflow:loss = 1.4311287, step = 11901 (1.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.2223\n",
      "INFO:tensorflow:loss = 1.4108648, step = 12001 (1.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.8881\n",
      "INFO:tensorflow:loss = 1.4500799, step = 12101 (1.301 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.2304\n",
      "INFO:tensorflow:loss = 1.7482464, step = 12201 (1.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.2432\n",
      "INFO:tensorflow:loss = 1.5528615, step = 12301 (1.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9394\n",
      "INFO:tensorflow:loss = 1.2892369, step = 12401 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1801\n",
      "INFO:tensorflow:loss = 1.455091, step = 12501 (1.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.002\n",
      "INFO:tensorflow:loss = 1.5161185, step = 12601 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2272\n",
      "INFO:tensorflow:loss = 1.7760108, step = 12701 (1.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.8373\n",
      "INFO:tensorflow:loss = 1.5564282, step = 12801 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.699\n",
      "INFO:tensorflow:loss = 1.3542686, step = 12901 (1.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6244\n",
      "INFO:tensorflow:loss = 1.5115279, step = 13001 (1.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0281\n",
      "INFO:tensorflow:loss = 1.1731701, step = 13101 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.6803\n",
      "INFO:tensorflow:loss = 1.2582611, step = 13201 (1.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.9917\n",
      "INFO:tensorflow:loss = 1.4751081, step = 13301 (1.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.1222\n",
      "INFO:tensorflow:loss = 1.8455474, step = 13401 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.8843\n",
      "INFO:tensorflow:loss = 1.665103, step = 13501 (1.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5159\n",
      "INFO:tensorflow:loss = 1.3562491, step = 13601 (1.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.763\n",
      "INFO:tensorflow:loss = 1.5706946, step = 13701 (1.318 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.336\n",
      "INFO:tensorflow:loss = 1.3508863, step = 13801 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2419\n",
      "INFO:tensorflow:loss = 1.4656383, step = 13901 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2515\n",
      "INFO:tensorflow:loss = 1.6515191, step = 14001 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6706\n",
      "INFO:tensorflow:loss = 1.5616541, step = 14101 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.385\n",
      "INFO:tensorflow:loss = 1.4963323, step = 14201 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.66\n",
      "INFO:tensorflow:loss = 1.3969518, step = 14301 (1.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.298\n",
      "INFO:tensorflow:loss = 1.4467093, step = 14401 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3408\n",
      "INFO:tensorflow:loss = 1.4609696, step = 14501 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3516\n",
      "INFO:tensorflow:loss = 1.7593892, step = 14601 (1.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.8274\n",
      "INFO:tensorflow:loss = 1.2640842, step = 14701 (1.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7722\n",
      "INFO:tensorflow:loss = 1.5548985, step = 14801 (1.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6643\n",
      "INFO:tensorflow:loss = 1.3379666, step = 14901 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2373\n",
      "INFO:tensorflow:loss = 1.3282868, step = 15001 (1.146 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.5869\n",
      "INFO:tensorflow:loss = 1.3730294, step = 15101 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9126\n",
      "INFO:tensorflow:loss = 1.2612957, step = 15201 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7464\n",
      "INFO:tensorflow:loss = 1.7517644, step = 15301 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8285\n",
      "INFO:tensorflow:loss = 1.5676782, step = 15401 (1.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3206\n",
      "INFO:tensorflow:loss = 1.476816, step = 15501 (1.215 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 79.7711\n",
      "INFO:tensorflow:loss = 1.7432317, step = 15601 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3704\n",
      "INFO:tensorflow:loss = 1.5274211, step = 15701 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0635\n",
      "INFO:tensorflow:loss = 1.5615784, step = 15801 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5191\n",
      "INFO:tensorflow:loss = 1.5279052, step = 15901 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0711\n",
      "INFO:tensorflow:loss = 1.5058185, step = 16001 (1.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2962\n",
      "INFO:tensorflow:loss = 1.6916344, step = 16101 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.6751\n",
      "INFO:tensorflow:loss = 1.4091972, step = 16201 (1.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.827\n",
      "INFO:tensorflow:loss = 1.34786, step = 16301 (1.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3577\n",
      "INFO:tensorflow:loss = 1.4632357, step = 16401 (1.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4021\n",
      "INFO:tensorflow:loss = 1.475768, step = 16501 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 98.0169\n",
      "INFO:tensorflow:loss = 1.7298355, step = 16601 (1.019 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.1072\n",
      "INFO:tensorflow:loss = 1.4030235, step = 16701 (1.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9027\n",
      "INFO:tensorflow:loss = 1.473855, step = 16801 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.7787\n",
      "INFO:tensorflow:loss = 1.3779733, step = 16901 (1.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.9232\n",
      "INFO:tensorflow:loss = 1.4042047, step = 17001 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3371\n",
      "INFO:tensorflow:loss = 1.4432598, step = 17101 (1.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3935\n",
      "INFO:tensorflow:loss = 1.3814586, step = 17201 (1.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 99.6902\n",
      "INFO:tensorflow:loss = 1.36317, step = 17301 (1.005 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.4379\n",
      "INFO:tensorflow:loss = 1.3068335, step = 17401 (1.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.962\n",
      "INFO:tensorflow:loss = 1.3317047, step = 17501 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.9782\n",
      "INFO:tensorflow:loss = 1.4049145, step = 17601 (1.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.576\n",
      "INFO:tensorflow:loss = 1.5430075, step = 17701 (1.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.2374\n",
      "INFO:tensorflow:loss = 1.2406958, step = 17801 (1.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6809\n",
      "INFO:tensorflow:loss = 1.493588, step = 17901 (1.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2827\n",
      "INFO:tensorflow:loss = 1.6237408, step = 18001 (1.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.8429\n",
      "INFO:tensorflow:loss = 1.5600786, step = 18101 (1.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3305\n",
      "INFO:tensorflow:loss = 1.0121888, step = 18201 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.8277\n",
      "INFO:tensorflow:loss = 1.6536597, step = 18301 (1.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.6111\n",
      "INFO:tensorflow:loss = 1.6160916, step = 18401 (1.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4585\n",
      "INFO:tensorflow:loss = 1.4393005, step = 18501 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3717\n",
      "INFO:tensorflow:loss = 1.4977666, step = 18601 (1.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.9713\n",
      "INFO:tensorflow:loss = 1.4647374, step = 18701 (1.098 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5245\n",
      "INFO:tensorflow:loss = 1.8123928, step = 18801 (1.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5258\n",
      "INFO:tensorflow:loss = 1.4830889, step = 18901 (1.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3146\n",
      "INFO:tensorflow:loss = 1.2898428, step = 19001 (1.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.2393\n",
      "INFO:tensorflow:loss = 1.412052, step = 19101 (1.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.3333\n",
      "INFO:tensorflow:loss = 1.50761, step = 19201 (1.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3216\n",
      "INFO:tensorflow:loss = 1.4219894, step = 19301 (1.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.6972\n",
      "INFO:tensorflow:loss = 1.2749485, step = 19401 (1.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2464\n",
      "INFO:tensorflow:loss = 1.4007405, step = 19501 (1.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5857\n",
      "INFO:tensorflow:loss = 1.5487683, step = 19601 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.9971\n",
      "INFO:tensorflow:loss = 1.4569932, step = 19701 (1.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4141\n",
      "INFO:tensorflow:loss = 1.5041219, step = 19801 (1.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0584\n",
      "INFO:tensorflow:loss = 1.5508106, step = 19901 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4878\n",
      "INFO:tensorflow:loss = 1.3807529, step = 20001 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2296\n",
      "INFO:tensorflow:loss = 1.2221192, step = 20101 (1.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 95.6269\n",
      "INFO:tensorflow:loss = 1.3399941, step = 20201 (1.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.0453\n",
      "INFO:tensorflow:loss = 1.257005, step = 20301 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4329\n",
      "INFO:tensorflow:loss = 1.4954299, step = 20401 (1.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.6839\n",
      "INFO:tensorflow:loss = 1.5292324, step = 20501 (1.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5357\n",
      "INFO:tensorflow:loss = 1.4894371, step = 20601 (1.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5287\n",
      "INFO:tensorflow:loss = 1.5193027, step = 20701 (1.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.5731\n",
      "INFO:tensorflow:loss = 1.4535087, step = 20801 (1.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0278\n",
      "INFO:tensorflow:loss = 1.2005888, step = 20901 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.8405\n",
      "INFO:tensorflow:loss = 1.4923394, step = 21001 (1.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3037\n",
      "INFO:tensorflow:loss = 1.2598747, step = 21101 (1.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3443\n",
      "INFO:tensorflow:loss = 1.338802, step = 21201 (1.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4222\n",
      "INFO:tensorflow:loss = 1.5623137, step = 21301 (1.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6842\n",
      "INFO:tensorflow:loss = 1.6243482, step = 21401 (1.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.8001\n",
      "INFO:tensorflow:loss = 1.6268944, step = 21501 (1.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.4703\n",
      "INFO:tensorflow:loss = 1.3549417, step = 21601 (1.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0152\n",
      "INFO:tensorflow:loss = 1.3667963, step = 21701 (1.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7749\n",
      "INFO:tensorflow:loss = 1.5993923, step = 21801 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.706\n",
      "INFO:tensorflow:loss = 1.4984857, step = 21901 (1.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.4713\n",
      "INFO:tensorflow:loss = 1.1000913, step = 22001 (1.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5521\n",
      "INFO:tensorflow:loss = 1.2183738, step = 22101 (1.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.0542\n",
      "INFO:tensorflow:loss = 1.5746847, step = 22201 (1.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.1789\n",
      "INFO:tensorflow:loss = 1.3577348, step = 22301 (1.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.0492\n",
      "INFO:tensorflow:loss = 1.1897235, step = 22401 (1.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2745\n",
      "INFO:tensorflow:loss = 1.6115003, step = 22501 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6014\n",
      "INFO:tensorflow:loss = 1.5819823, step = 22601 (1.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2531\n",
      "INFO:tensorflow:loss = 1.333119, step = 22701 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7661\n",
      "INFO:tensorflow:loss = 1.5073503, step = 22801 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.9558\n",
      "INFO:tensorflow:loss = 1.5871161, step = 22901 (1.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.1657\n",
      "INFO:tensorflow:loss = 1.6540414, step = 23001 (1.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2272\n",
      "INFO:tensorflow:loss = 1.2863553, step = 23101 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.075\n",
      "INFO:tensorflow:loss = 1.4513183, step = 23201 (1.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7563\n",
      "INFO:tensorflow:loss = 1.7017798, step = 23301 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.1056\n",
      "INFO:tensorflow:loss = 1.2207501, step = 23401 (1.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.8113\n",
      "INFO:tensorflow:loss = 1.5117912, step = 23501 (1.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.3648\n",
      "INFO:tensorflow:loss = 1.2992378, step = 23601 (1.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.5571\n",
      "INFO:tensorflow:loss = 1.3410308, step = 23701 (1.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.4857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.5827935, step = 23801 (1.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7718\n",
      "INFO:tensorflow:loss = 1.2573701, step = 23901 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.6336\n",
      "INFO:tensorflow:loss = 1.4363369, step = 24001 (1.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9698\n",
      "INFO:tensorflow:loss = 1.5365548, step = 24101 (1.252 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1115\n",
      "INFO:tensorflow:loss = 1.4149735, step = 24201 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.1111\n",
      "INFO:tensorflow:loss = 1.4718205, step = 24301 (1.296 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.9049\n",
      "INFO:tensorflow:loss = 1.2519189, step = 24401 (1.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2325\n",
      "INFO:tensorflow:loss = 1.5885701, step = 24501 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.2118\n",
      "INFO:tensorflow:loss = 1.3978456, step = 24601 (1.187 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8019\n",
      "INFO:tensorflow:loss = 1.4328742, step = 24701 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.7414\n",
      "INFO:tensorflow:loss = 1.4261571, step = 24801 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.2721\n",
      "INFO:tensorflow:loss = 1.3789542, step = 24901 (1.529 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.0774\n",
      "INFO:tensorflow:loss = 1.4166952, step = 25001 (1.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0133\n",
      "INFO:tensorflow:loss = 1.6112517, step = 25101 (1.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.9536\n",
      "INFO:tensorflow:loss = 1.2854445, step = 25201 (1.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1299\n",
      "INFO:tensorflow:loss = 1.2169385, step = 25301 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.8909\n",
      "INFO:tensorflow:loss = 1.407176, step = 25401 (1.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4931\n",
      "INFO:tensorflow:loss = 1.3692062, step = 25501 (1.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.4178\n",
      "INFO:tensorflow:loss = 1.8731537, step = 25601 (1.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9838\n",
      "INFO:tensorflow:loss = 1.3539356, step = 25701 (1.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7137\n",
      "INFO:tensorflow:loss = 1.485825, step = 25801 (1.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6031\n",
      "INFO:tensorflow:loss = 1.4143256, step = 25901 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3384\n",
      "INFO:tensorflow:loss = 1.4106112, step = 26001 (1.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9247\n",
      "INFO:tensorflow:loss = 1.3778872, step = 26101 (1.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1233\n",
      "INFO:tensorflow:loss = 1.1939868, step = 26201 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.167\n",
      "INFO:tensorflow:loss = 1.609632, step = 26301 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8869\n",
      "INFO:tensorflow:loss = 1.4639406, step = 26401 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.7649\n",
      "INFO:tensorflow:loss = 1.3133053, step = 26501 (1.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.2538\n",
      "INFO:tensorflow:loss = 1.6515126, step = 26601 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5552\n",
      "INFO:tensorflow:loss = 1.2071041, step = 26701 (1.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0315\n",
      "INFO:tensorflow:loss = 1.3746611, step = 26801 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.923\n",
      "INFO:tensorflow:loss = 1.3496362, step = 26901 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7524\n",
      "INFO:tensorflow:loss = 1.4620118, step = 27001 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.4939\n",
      "INFO:tensorflow:loss = 1.3300744, step = 27101 (1.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2317\n",
      "INFO:tensorflow:loss = 1.5770985, step = 27201 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1744\n",
      "INFO:tensorflow:loss = 1.3487327, step = 27301 (1.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6252\n",
      "INFO:tensorflow:loss = 1.3186505, step = 27401 (1.249 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9416\n",
      "INFO:tensorflow:loss = 1.4934824, step = 27501 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4933\n",
      "INFO:tensorflow:loss = 1.6131161, step = 27601 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.5037\n",
      "INFO:tensorflow:loss = 1.5197188, step = 27701 (1.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.8851\n",
      "INFO:tensorflow:loss = 1.3861871, step = 27801 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1329\n",
      "INFO:tensorflow:loss = 1.2188151, step = 27901 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6329\n",
      "INFO:tensorflow:loss = 1.4469963, step = 28001 (1.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.0842\n",
      "INFO:tensorflow:loss = 1.6035804, step = 28101 (1.298 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5587\n",
      "INFO:tensorflow:loss = 1.3583719, step = 28201 (1.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.4487\n",
      "INFO:tensorflow:loss = 1.0166066, step = 28301 (1.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5768\n",
      "INFO:tensorflow:loss = 1.4774708, step = 28401 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4654\n",
      "INFO:tensorflow:loss = 1.4340484, step = 28501 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.5244\n",
      "INFO:tensorflow:loss = 1.5271971, step = 28601 (1.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0873\n",
      "INFO:tensorflow:loss = 1.3228745, step = 28701 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7206\n",
      "INFO:tensorflow:loss = 1.5699055, step = 28801 (1.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3234\n",
      "INFO:tensorflow:loss = 1.2155426, step = 28901 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0238\n",
      "INFO:tensorflow:loss = 1.4406627, step = 29001 (1.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.8323\n",
      "INFO:tensorflow:loss = 1.617628, step = 29101 (1.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.439\n",
      "INFO:tensorflow:loss = 1.2104063, step = 29201 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.0416\n",
      "INFO:tensorflow:loss = 1.3551745, step = 29301 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1678\n",
      "INFO:tensorflow:loss = 1.2381425, step = 29401 (1.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1505\n",
      "INFO:tensorflow:loss = 1.1767331, step = 29501 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7388\n",
      "INFO:tensorflow:loss = 1.2428272, step = 29601 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.2541\n",
      "INFO:tensorflow:loss = 1.2201952, step = 29701 (1.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0383\n",
      "INFO:tensorflow:loss = 1.3792491, step = 29801 (1.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1917\n",
      "INFO:tensorflow:loss = 1.504799, step = 29901 (1.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.4961\n",
      "INFO:tensorflow:loss = 1.398385, step = 30001 (1.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1018\n",
      "INFO:tensorflow:loss = 1.3960173, step = 30101 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8371\n",
      "INFO:tensorflow:loss = 1.1686156, step = 30201 (1.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.7313\n",
      "INFO:tensorflow:loss = 1.8328518, step = 30301 (1.304 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.4219\n",
      "INFO:tensorflow:loss = 1.2422789, step = 30401 (1.420 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.352\n",
      "INFO:tensorflow:loss = 1.4898151, step = 30501 (1.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.1595\n",
      "INFO:tensorflow:loss = 1.225098, step = 30601 (1.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.4793\n",
      "INFO:tensorflow:loss = 1.3997725, step = 30701 (1.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.064\n",
      "INFO:tensorflow:loss = 1.556944, step = 30801 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.4267\n",
      "INFO:tensorflow:loss = 1.1518308, step = 30901 (1.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.3907\n",
      "INFO:tensorflow:loss = 1.4911766, step = 31001 (1.564 sec)\n",
      "INFO:tensorflow:global_step/sec: 68.5705\n",
      "INFO:tensorflow:loss = 1.3357408, step = 31101 (1.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.3771\n",
      "INFO:tensorflow:loss = 1.2242042, step = 31201 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.4671\n",
      "INFO:tensorflow:loss = 1.3720207, step = 31301 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4468\n",
      "INFO:tensorflow:loss = 1.5981011, step = 31401 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.7269\n",
      "INFO:tensorflow:loss = 1.3047756, step = 31501 (1.596 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9921\n",
      "INFO:tensorflow:loss = 1.2572132, step = 31601 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4726\n",
      "INFO:tensorflow:loss = 1.354131, step = 31701 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9765\n",
      "INFO:tensorflow:loss = 1.4262762, step = 31801 (1.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3869\n",
      "INFO:tensorflow:loss = 1.4978108, step = 31901 (1.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.1532\n",
      "INFO:tensorflow:loss = 1.5266957, step = 32001 (1.320 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 81.5913\n",
      "INFO:tensorflow:loss = 1.7639614, step = 32101 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.7616\n",
      "INFO:tensorflow:loss = 1.4361664, step = 32201 (1.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 66.3404\n",
      "INFO:tensorflow:loss = 1.004593, step = 32301 (1.507 sec)\n",
      "INFO:tensorflow:global_step/sec: 71.487\n",
      "INFO:tensorflow:loss = 1.2968527, step = 32401 (1.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 65.7192\n",
      "INFO:tensorflow:loss = 1.5405825, step = 32501 (1.523 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.3435\n",
      "INFO:tensorflow:loss = 1.3609086, step = 32601 (1.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.7574\n",
      "INFO:tensorflow:loss = 1.5100386, step = 32701 (1.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.0636\n",
      "INFO:tensorflow:loss = 1.5995985, step = 32801 (1.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8738\n",
      "INFO:tensorflow:loss = 1.607801, step = 32901 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8868\n",
      "INFO:tensorflow:loss = 1.4337857, step = 33001 (1.192 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6972\n",
      "INFO:tensorflow:loss = 1.2500522, step = 33101 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.3388\n",
      "INFO:tensorflow:loss = 1.4494287, step = 33201 (1.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.941\n",
      "INFO:tensorflow:loss = 1.3261464, step = 33301 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.5313\n",
      "INFO:tensorflow:loss = 1.6102198, step = 33401 (1.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.4773\n",
      "INFO:tensorflow:loss = 1.1529906, step = 33501 (1.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4578\n",
      "INFO:tensorflow:loss = 1.6349837, step = 33601 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4472\n",
      "INFO:tensorflow:loss = 1.4673343, step = 33701 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8065\n",
      "INFO:tensorflow:loss = 1.0295255, step = 33801 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9185\n",
      "INFO:tensorflow:loss = 1.4017929, step = 33901 (1.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3736\n",
      "INFO:tensorflow:loss = 1.5490844, step = 34001 (1.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1619\n",
      "INFO:tensorflow:loss = 1.6431924, step = 34101 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3839\n",
      "INFO:tensorflow:loss = 1.5906165, step = 34201 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.9468\n",
      "INFO:tensorflow:loss = 1.2796135, step = 34301 (1.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.0451\n",
      "INFO:tensorflow:loss = 1.758042, step = 34401 (1.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.399\n",
      "INFO:tensorflow:loss = 1.2684639, step = 34501 (1.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7439\n",
      "INFO:tensorflow:loss = 1.552374, step = 34601 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9651\n",
      "INFO:tensorflow:loss = 1.2999469, step = 34701 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7975\n",
      "INFO:tensorflow:loss = 1.4375513, step = 34801 (1.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5093\n",
      "INFO:tensorflow:loss = 1.4540573, step = 34901 (1.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.9436\n",
      "INFO:tensorflow:loss = 1.6585302, step = 35001 (1.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6448\n",
      "INFO:tensorflow:loss = 1.5798666, step = 35101 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2144\n",
      "INFO:tensorflow:loss = 1.2988056, step = 35201 (1.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 91.7698\n",
      "INFO:tensorflow:loss = 1.5388236, step = 35301 (1.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.8857\n",
      "INFO:tensorflow:loss = 1.2660327, step = 35401 (1.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9135\n",
      "INFO:tensorflow:loss = 1.5436155, step = 35501 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.3782\n",
      "INFO:tensorflow:loss = 1.3272662, step = 35601 (1.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.3449\n",
      "INFO:tensorflow:loss = 1.16118, step = 35701 (1.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.5188\n",
      "INFO:tensorflow:loss = 1.3490987, step = 35801 (1.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3807\n",
      "INFO:tensorflow:loss = 1.5780406, step = 35901 (1.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.4396\n",
      "INFO:tensorflow:loss = 1.5152786, step = 36001 (1.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.0728\n",
      "INFO:tensorflow:loss = 1.3059028, step = 36101 (1.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.4253\n",
      "INFO:tensorflow:loss = 1.4926028, step = 36201 (1.485 sec)\n",
      "INFO:tensorflow:global_step/sec: 61.0199\n",
      "INFO:tensorflow:loss = 1.283748, step = 36301 (1.637 sec)\n",
      "INFO:tensorflow:global_step/sec: 57.8531\n",
      "INFO:tensorflow:loss = 1.127038, step = 36401 (1.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 67.3954\n",
      "INFO:tensorflow:loss = 1.3851892, step = 36501 (1.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 69.4007\n",
      "INFO:tensorflow:loss = 1.4852535, step = 36601 (1.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.7708\n",
      "INFO:tensorflow:loss = 1.3444676, step = 36701 (1.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.3531\n",
      "INFO:tensorflow:loss = 1.3774517, step = 36801 (1.377 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.0397\n",
      "INFO:tensorflow:loss = 1.4607266, step = 36901 (1.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.6717\n",
      "INFO:tensorflow:loss = 1.5191298, step = 37001 (1.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9025\n",
      "INFO:tensorflow:loss = 1.302298, step = 37101 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1754\n",
      "INFO:tensorflow:loss = 1.2825501, step = 37201 (1.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3677\n",
      "INFO:tensorflow:loss = 1.4556689, step = 37301 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9838\n",
      "INFO:tensorflow:loss = 1.5439553, step = 37401 (1.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.0218\n",
      "INFO:tensorflow:loss = 1.5704118, step = 37501 (1.282 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.6978\n",
      "INFO:tensorflow:loss = 1.3220502, step = 37601 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4843\n",
      "INFO:tensorflow:loss = 1.1624467, step = 37701 (1.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.7666\n",
      "INFO:tensorflow:loss = 1.3566874, step = 37801 (1.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.0615\n",
      "INFO:tensorflow:loss = 1.5199585, step = 37901 (1.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9007\n",
      "INFO:tensorflow:loss = 1.6344229, step = 38001 (1.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1945\n",
      "INFO:tensorflow:loss = 1.394965, step = 38101 (1.261 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.6256\n",
      "INFO:tensorflow:loss = 1.2842643, step = 38201 (1.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.7779\n",
      "INFO:tensorflow:loss = 1.301286, step = 38301 (1.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 70.289\n",
      "INFO:tensorflow:loss = 1.1983109, step = 38401 (1.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.9226\n",
      "INFO:tensorflow:loss = 1.4100915, step = 38501 (1.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4525\n",
      "INFO:tensorflow:loss = 1.3743949, step = 38601 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3472\n",
      "INFO:tensorflow:loss = 1.5001891, step = 38701 (1.244 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.0881\n",
      "INFO:tensorflow:loss = 1.4751962, step = 38801 (1.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.3729\n",
      "INFO:tensorflow:loss = 1.4169661, step = 38901 (1.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3975\n",
      "INFO:tensorflow:loss = 1.1937195, step = 39001 (1.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5897\n",
      "INFO:tensorflow:loss = 1.5249798, step = 39101 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.5031\n",
      "INFO:tensorflow:loss = 1.5007931, step = 39201 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1559\n",
      "INFO:tensorflow:loss = 1.3776495, step = 39301 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.3511\n",
      "INFO:tensorflow:loss = 1.214392, step = 39401 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.3343\n",
      "INFO:tensorflow:loss = 1.2013886, step = 39501 (1.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7679\n",
      "INFO:tensorflow:loss = 1.4389545, step = 39601 (1.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.009\n",
      "INFO:tensorflow:loss = 1.3921912, step = 39701 (1.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5572\n",
      "INFO:tensorflow:loss = 1.3243165, step = 39801 (1.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.7545\n",
      "INFO:tensorflow:loss = 1.1481853, step = 39901 (1.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3396\n",
      "INFO:tensorflow:loss = 1.3467772, step = 40001 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.5242\n",
      "INFO:tensorflow:loss = 1.4016385, step = 40101 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.605\n",
      "INFO:tensorflow:loss = 1.3090189, step = 40201 (1.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.4567295, step = 40301 (1.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.7169\n",
      "INFO:tensorflow:loss = 1.2460548, step = 40401 (1.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.6747\n",
      "INFO:tensorflow:loss = 1.3704927, step = 40501 (1.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3192\n",
      "INFO:tensorflow:loss = 1.4358886, step = 40601 (1.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8321\n",
      "INFO:tensorflow:loss = 1.272635, step = 40701 (1.152 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.497\n",
      "INFO:tensorflow:loss = 1.2160566, step = 40801 (1.142 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.1778\n",
      "INFO:tensorflow:loss = 1.4795069, step = 40901 (1.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5785\n",
      "INFO:tensorflow:loss = 1.1096754, step = 41001 (1.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8753\n",
      "INFO:tensorflow:loss = 1.4880339, step = 41101 (1.190 sec)\n",
      "INFO:tensorflow:global_step/sec: 74.8\n",
      "INFO:tensorflow:loss = 1.4305333, step = 41201 (1.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 63.4775\n",
      "INFO:tensorflow:loss = 1.3045975, step = 41301 (1.574 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5585\n",
      "INFO:tensorflow:loss = 1.2966723, step = 41401 (1.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0257\n",
      "INFO:tensorflow:loss = 1.1581409, step = 41501 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.2648\n",
      "INFO:tensorflow:loss = 1.4702103, step = 41601 (1.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3783\n",
      "INFO:tensorflow:loss = 1.3251721, step = 41701 (1.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.1353\n",
      "INFO:tensorflow:loss = 1.309929, step = 41801 (1.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5793\n",
      "INFO:tensorflow:loss = 1.5417595, step = 41901 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.5154\n",
      "INFO:tensorflow:loss = 1.7466806, step = 42001 (1.306 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6721\n",
      "INFO:tensorflow:loss = 1.2203385, step = 42101 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.656\n",
      "INFO:tensorflow:loss = 1.426716, step = 42201 (1.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.6404\n",
      "INFO:tensorflow:loss = 1.3330988, step = 42301 (1.290 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0117\n",
      "INFO:tensorflow:loss = 1.0744948, step = 42401 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5538\n",
      "INFO:tensorflow:loss = 1.3816609, step = 42501 (1.141 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8469\n",
      "INFO:tensorflow:loss = 1.6178836, step = 42601 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8844\n",
      "INFO:tensorflow:loss = 1.3865921, step = 42701 (1.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1167\n",
      "INFO:tensorflow:loss = 1.5367634, step = 42801 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.0131\n",
      "INFO:tensorflow:loss = 1.4127384, step = 42901 (1.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.409\n",
      "INFO:tensorflow:loss = 1.3710974, step = 43001 (1.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.1158\n",
      "INFO:tensorflow:loss = 1.4048178, step = 43101 (1.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7659\n",
      "INFO:tensorflow:loss = 1.2722553, step = 43201 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7354\n",
      "INFO:tensorflow:loss = 1.0442328, step = 43301 (1.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.8454\n",
      "INFO:tensorflow:loss = 1.4913168, step = 43401 (1.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7367\n",
      "INFO:tensorflow:loss = 1.5341852, step = 43501 (1.139 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.8572\n",
      "INFO:tensorflow:loss = 1.304374, step = 43601 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4943\n",
      "INFO:tensorflow:loss = 1.491981, step = 43701 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7676\n",
      "INFO:tensorflow:loss = 1.1272607, step = 43801 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7499\n",
      "INFO:tensorflow:loss = 1.2383192, step = 43901 (1.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.9658\n",
      "INFO:tensorflow:loss = 1.3377625, step = 44001 (1.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7407\n",
      "INFO:tensorflow:loss = 1.2780087, step = 44101 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.4634\n",
      "INFO:tensorflow:loss = 1.4000899, step = 44201 (1.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.59\n",
      "INFO:tensorflow:loss = 1.5492357, step = 44301 (1.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.3946\n",
      "INFO:tensorflow:loss = 1.3838385, step = 44401 (1.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7872\n",
      "INFO:tensorflow:loss = 1.3204805, step = 44501 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3016\n",
      "INFO:tensorflow:loss = 1.2410591, step = 44601 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7124\n",
      "INFO:tensorflow:loss = 1.4799539, step = 44701 (1.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.5289\n",
      "INFO:tensorflow:loss = 1.1702472, step = 44801 (1.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.2169\n",
      "INFO:tensorflow:loss = 1.3461071, step = 44901 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.7175\n",
      "INFO:tensorflow:loss = 1.2929361, step = 45001 (1.137 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7027\n",
      "INFO:tensorflow:loss = 1.4668796, step = 45101 (1.155 sec)\n",
      "INFO:tensorflow:global_step/sec: 90.421\n",
      "INFO:tensorflow:loss = 1.2454934, step = 45201 (1.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.079\n",
      "INFO:tensorflow:loss = 1.5313041, step = 45301 (1.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6556\n",
      "INFO:tensorflow:loss = 1.3280416, step = 45401 (1.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5746\n",
      "INFO:tensorflow:loss = 1.2306976, step = 45501 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.4188\n",
      "INFO:tensorflow:loss = 1.2312505, step = 45601 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5749\n",
      "INFO:tensorflow:loss = 1.428128, step = 45701 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7826\n",
      "INFO:tensorflow:loss = 1.6128356, step = 45801 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.213\n",
      "INFO:tensorflow:loss = 1.7642827, step = 45901 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.5488\n",
      "INFO:tensorflow:loss = 1.2376833, step = 46001 (1.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0441\n",
      "INFO:tensorflow:loss = 1.227494, step = 46101 (1.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7037\n",
      "INFO:tensorflow:loss = 1.4697518, step = 46201 (1.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1389\n",
      "INFO:tensorflow:loss = 1.038007, step = 46301 (1.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2825\n",
      "INFO:tensorflow:loss = 1.4314343, step = 46401 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5394\n",
      "INFO:tensorflow:loss = 1.3943048, step = 46501 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.058\n",
      "INFO:tensorflow:loss = 1.3174472, step = 46601 (1.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.4163\n",
      "INFO:tensorflow:loss = 1.1957257, step = 46701 (1.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.4379\n",
      "INFO:tensorflow:loss = 1.1429789, step = 46801 (1.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.405\n",
      "INFO:tensorflow:loss = 1.354179, step = 46901 (1.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.7282\n",
      "INFO:tensorflow:loss = 1.2514557, step = 47001 (1.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0608\n",
      "INFO:tensorflow:loss = 1.2310902, step = 47101 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6205\n",
      "INFO:tensorflow:loss = 1.3628502, step = 47201 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.6626\n",
      "INFO:tensorflow:loss = 1.1945609, step = 47301 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.9112\n",
      "INFO:tensorflow:loss = 1.3867389, step = 47401 (1.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0067\n",
      "INFO:tensorflow:loss = 1.4758295, step = 47501 (1.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9809\n",
      "INFO:tensorflow:loss = 1.0590749, step = 47601 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.1882\n",
      "INFO:tensorflow:loss = 1.6668168, step = 47701 (1.134 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.4418\n",
      "INFO:tensorflow:loss = 1.3675697, step = 47801 (1.292 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8676\n",
      "INFO:tensorflow:loss = 1.3144113, step = 47901 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1785\n",
      "INFO:tensorflow:loss = 1.306518, step = 48001 (1.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.9904\n",
      "INFO:tensorflow:loss = 1.482395, step = 48101 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.7945\n",
      "INFO:tensorflow:loss = 1.2440425, step = 48201 (1.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1262\n",
      "INFO:tensorflow:loss = 1.437839, step = 48301 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.498\n",
      "INFO:tensorflow:loss = 1.4405212, step = 48401 (1.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.9509\n",
      "INFO:tensorflow:loss = 1.3270363, step = 48501 (1.162 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 89.1089\n",
      "INFO:tensorflow:loss = 1.1326135, step = 48601 (1.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.5756\n",
      "INFO:tensorflow:loss = 1.206904, step = 48701 (1.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.1412\n",
      "INFO:tensorflow:loss = 1.2497197, step = 48801 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.5758\n",
      "INFO:tensorflow:loss = 1.3636596, step = 48901 (1.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7143\n",
      "INFO:tensorflow:loss = 1.2415161, step = 49001 (1.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 73.5742\n",
      "INFO:tensorflow:loss = 1.4068776, step = 49101 (1.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 89.6046\n",
      "INFO:tensorflow:loss = 1.1427858, step = 49201 (1.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.916\n",
      "INFO:tensorflow:loss = 1.5131798, step = 49301 (1.151 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.2066\n",
      "INFO:tensorflow:loss = 1.1927297, step = 49401 (1.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8657\n",
      "INFO:tensorflow:loss = 1.414427, step = 49501 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3912\n",
      "INFO:tensorflow:loss = 1.200426, step = 49601 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.668\n",
      "INFO:tensorflow:loss = 1.2889007, step = 49701 (1.287 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.5303\n",
      "INFO:tensorflow:loss = 1.3728093, step = 49801 (1.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.198\n",
      "INFO:tensorflow:loss = 1.4052173, step = 49901 (1.218 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 50000 into /tmp/tmpdu3z1poh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.4758295.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SKCompat()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there are 14 different airlines, we need to change n_classes to 14, default seems to be 10\n",
    "# NOTE: This will take a few minutes!!\n",
    "# ALSO: The more steps you add, the better the accuracy is, however, we are going to put steps at 50,000\n",
    "# so that it will not be an unreasonably long wait time!\n",
    "# Steps = 100,000 equals 58% accuracy\n",
    "\n",
    "feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(x_train)\n",
    "dnn_clf_hidden = tf.contrib.learn.DNNClassifier(hidden_units = [300, 150], n_classes = 14, feature_columns = feature_cols)\n",
    "dnn_clf_hidden = tf.contrib.learn.SKCompat(dnn_clf_hidden)\n",
    "dnn_clf_hidden.fit(x_train, y_train_int, batch_size = 50, steps = 50000) \n",
    "# 10,000 (46%); 15,000 (47%), 40,000 (50%); 50,000 (53%); 100,000 (58%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpdu3z1poh/model.ckpt-50000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5275538415660428"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the class accuracy score\n",
    "\n",
    "y_predict_hidden = dnn_clf_hidden.predict(x_test)\n",
    "accuracy_score(y_test_int, y_predict_hidden['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpdu3z1poh/model.ckpt-50000\n",
      "Confusion Matrix: \n",
      "\n",
      " [[12685   245   188  2673   715    28   118   388   295   717  1737  1130\n",
      "     53  3770]\n",
      " [  291  1114    34   220   177     0    25   142   413   762   462     8\n",
      "      5  2214]\n",
      " [ 1044   373   784  1275   427    79     6    79   591   211  1308   176\n",
      "     25  2679]\n",
      " [ 2098   169   230 18175  1051    92    50   182   227   905  2152   196\n",
      "      1  4050]\n",
      " [   14     7    16   447 15158     3     0   482    65  1804    41    15\n",
      "      0  1218]\n",
      " [   36     4    11   683   214   486     0    27     2   183   165    32\n",
      "      0  1132]\n",
      " [   34    73     0   140    83     0  1851    53     0   202   132     0\n",
      "      9    15]\n",
      " [   75    19     4    80   217     0    19  7456     2   443     2     0\n",
      "      0  1733]\n",
      " [  299    56    94   100    50     0     8   132   789   325   345    42\n",
      "      0  1835]\n",
      " [   51    13     2   161  4989     8    40   324   272 11002    41    15\n",
      "      0  3388]\n",
      " [ 2068   543   169  1837   512    76    39   263   385   962  6756   189\n",
      "      8  3811]\n",
      " [  779   165    98  1203   448     8     2    32   180   497   477  1344\n",
      "      0  1608]\n",
      " [  131    67    47   235    18     8     4     9   200   161   392     1\n",
      "     59   830]\n",
      " [ 2138   176   290  3974  1982   328    80  1459   469  3310  1602   182\n",
      "      9 26717]]\n"
     ]
    }
   ],
   "source": [
    "y_predict_hidden_test = dnn_clf_hidden.predict(x_test)\n",
    "\n",
    "c = tf.confusion_matrix(y_test_int, y_predict_hidden_test['classes'])\n",
    "\n",
    "with tf.Session():\n",
    "    print('Confusion Matrix: \\n\\n', tf.Tensor.eval(c,feed_dict=None, session=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out some Activation Functions\n",
    "\n",
    "Our fully connected network with 2 hidden layers didn't perform to well, so let's experiment with some activation functions!\n",
    "\n",
    "The first alternate activation function that we will try is the sigmoid, after than we will try the tanh; in both cases the accuracy decreased from the default relu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmps5uf202e\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f2b8f79bef0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/tmp/tmps5uf202e'}\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmps5uf202e/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.9307356, step = 1\n",
      "INFO:tensorflow:global_step/sec: 73.7265\n",
      "INFO:tensorflow:loss = 2.1337984, step = 101 (1.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5765\n",
      "INFO:tensorflow:loss = 2.200553, step = 201 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2132\n",
      "INFO:tensorflow:loss = 1.6905905, step = 301 (1.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0596\n",
      "INFO:tensorflow:loss = 2.0830734, step = 401 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.909\n",
      "INFO:tensorflow:loss = 1.9685929, step = 501 (1.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9935\n",
      "INFO:tensorflow:loss = 2.0397615, step = 601 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0631\n",
      "INFO:tensorflow:loss = 1.7566345, step = 701 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.2439\n",
      "INFO:tensorflow:loss = 2.1510246, step = 801 (1.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0973\n",
      "INFO:tensorflow:loss = 1.8526511, step = 901 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.378\n",
      "INFO:tensorflow:loss = 1.9369851, step = 1001 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2107\n",
      "INFO:tensorflow:loss = 1.8592277, step = 1101 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9684\n",
      "INFO:tensorflow:loss = 1.8315436, step = 1201 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3679\n",
      "INFO:tensorflow:loss = 1.8402915, step = 1301 (1.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.358\n",
      "INFO:tensorflow:loss = 1.9183093, step = 1401 (1.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.0838\n",
      "INFO:tensorflow:loss = 1.9015917, step = 1501 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.7671\n",
      "INFO:tensorflow:loss = 1.9044275, step = 1601 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.1195\n",
      "INFO:tensorflow:loss = 2.1203828, step = 1701 (1.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4989\n",
      "INFO:tensorflow:loss = 1.5491195, step = 1801 (1.257 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.7383\n",
      "INFO:tensorflow:loss = 1.7841166, step = 1901 (1.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6157\n",
      "INFO:tensorflow:loss = 1.8754895, step = 2001 (1.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.794\n",
      "INFO:tensorflow:loss = 1.9573778, step = 2101 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.7626\n",
      "INFO:tensorflow:loss = 2.0454917, step = 2201 (1.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.3686\n",
      "INFO:tensorflow:loss = 1.6014665, step = 2301 (1.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.8086\n",
      "INFO:tensorflow:loss = 1.8561002, step = 2401 (1.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 62.8172\n",
      "INFO:tensorflow:loss = 1.8878217, step = 2501 (1.591 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.4935\n",
      "INFO:tensorflow:loss = 1.6986091, step = 2601 (1.309 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1698\n",
      "INFO:tensorflow:loss = 1.9312308, step = 2701 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.7321\n",
      "INFO:tensorflow:loss = 1.937299, step = 2801 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2165\n",
      "INFO:tensorflow:loss = 2.0749252, step = 2901 (1.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.3903\n",
      "INFO:tensorflow:loss = 1.7506309, step = 3001 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2961\n",
      "INFO:tensorflow:loss = 2.1261861, step = 3101 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.6451\n",
      "INFO:tensorflow:loss = 1.7223306, step = 3201 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.5699\n",
      "INFO:tensorflow:loss = 1.592739, step = 3301 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1316\n",
      "INFO:tensorflow:loss = 1.7141777, step = 3401 (1.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2196\n",
      "INFO:tensorflow:loss = 1.8393451, step = 3501 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7237\n",
      "INFO:tensorflow:loss = 1.5991583, step = 3601 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1934\n",
      "INFO:tensorflow:loss = 2.0533068, step = 3701 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.5184\n",
      "INFO:tensorflow:loss = 1.7034366, step = 3801 (1.255 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8519\n",
      "INFO:tensorflow:loss = 1.7659, step = 3901 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.6211\n",
      "INFO:tensorflow:loss = 1.8411525, step = 4001 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 92.2577\n",
      "INFO:tensorflow:loss = 1.7225394, step = 4101 (1.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 101.063\n",
      "INFO:tensorflow:loss = 2.0707862, step = 4201 (0.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 103.273\n",
      "INFO:tensorflow:loss = 1.5020756, step = 4301 (0.968 sec)\n",
      "INFO:tensorflow:global_step/sec: 104.627\n",
      "INFO:tensorflow:loss = 1.6378031, step = 4401 (0.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1225\n",
      "INFO:tensorflow:loss = 1.7381934, step = 4501 (1.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9243\n",
      "INFO:tensorflow:loss = 1.8528855, step = 4601 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1817\n",
      "INFO:tensorflow:loss = 1.7664688, step = 4701 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5772\n",
      "INFO:tensorflow:loss = 1.8133627, step = 4801 (1.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4989\n",
      "INFO:tensorflow:loss = 1.8622806, step = 4901 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.7452\n",
      "INFO:tensorflow:loss = 1.5393125, step = 5001 (1.271 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.7576\n",
      "INFO:tensorflow:loss = 1.8221289, step = 5101 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.5813\n",
      "INFO:tensorflow:loss = 1.6594365, step = 5201 (1.143 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8444\n",
      "INFO:tensorflow:loss = 1.6204224, step = 5301 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.562\n",
      "INFO:tensorflow:loss = 1.6514754, step = 5401 (1.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.3694\n",
      "INFO:tensorflow:loss = 1.590522, step = 5501 (1.157 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.2034\n",
      "INFO:tensorflow:loss = 1.447786, step = 5601 (1.148 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.4086\n",
      "INFO:tensorflow:loss = 1.5155786, step = 5701 (1.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.3655\n",
      "INFO:tensorflow:loss = 1.4653965, step = 5801 (1.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.2595\n",
      "INFO:tensorflow:loss = 1.6764624, step = 5901 (1.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.6528\n",
      "INFO:tensorflow:loss = 1.6554804, step = 6001 (1.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0187\n",
      "INFO:tensorflow:loss = 2.0062935, step = 6101 (1.265 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.0187\n",
      "INFO:tensorflow:loss = 1.874905, step = 6201 (1.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.5892\n",
      "INFO:tensorflow:loss = 1.717482, step = 6301 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.3089\n",
      "INFO:tensorflow:loss = 1.8540945, step = 6401 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.0419\n",
      "INFO:tensorflow:loss = 1.6116215, step = 6501 (1.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8035\n",
      "INFO:tensorflow:loss = 1.6488765, step = 6601 (1.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.838\n",
      "INFO:tensorflow:loss = 1.6450372, step = 6701 (1.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 85.1599\n",
      "INFO:tensorflow:loss = 1.6453251, step = 6801 (1.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7623\n",
      "INFO:tensorflow:loss = 1.8942872, step = 6901 (1.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8873\n",
      "INFO:tensorflow:loss = 1.8049122, step = 7001 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 88.1859\n",
      "INFO:tensorflow:loss = 1.5707717, step = 7101 (1.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3981\n",
      "INFO:tensorflow:loss = 1.6103337, step = 7201 (1.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.6117\n",
      "INFO:tensorflow:loss = 1.864814, step = 7301 (1.159 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 80.0648\n",
      "INFO:tensorflow:loss = 1.3786496, step = 7401 (1.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.397\n",
      "INFO:tensorflow:loss = 1.51376, step = 7501 (1.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6307\n",
      "INFO:tensorflow:loss = 1.6467857, step = 7601 (1.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 87.0026\n",
      "INFO:tensorflow:loss = 1.8048129, step = 7701 (1.150 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.461\n",
      "INFO:tensorflow:loss = 1.7487508, step = 7801 (1.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.0188\n",
      "INFO:tensorflow:loss = 1.7537538, step = 7901 (1.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.9494\n",
      "INFO:tensorflow:loss = 1.9345204, step = 8001 (1.145 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6025\n",
      "INFO:tensorflow:loss = 1.7313089, step = 8101 (1.211 sec)\n"
     ]
    }
   ],
   "source": [
    "dnn_clf_sig = tf.contrib.learn.DNNClassifier(hidden_units = [300, 150], n_classes = 14, activation_fn=\"sigmoid\", feature_columns = feature_cols)\n",
    "dnn_clf_sig = tf.contrib.learn.SKCompat(dnn_clf_sig)\n",
    "dnn_clf_sig.fit(x_train, y_train_int, batch_size = 50, steps = 50000) # 10,000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_sig = dnn_clf_sig.predict(x_test)\n",
    "accuracy_score(y_test_int, y_predict_sig['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try out tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_clf_tanh = tf.contrib.learn.DNNClassifier(hidden_units = [300, 150], n_classes = 14, activation_fn=\"tanh\", feature_columns = feature_cols)\n",
    "dnn_clf_tanh = tf.contrib.learn.SKCompat(dnn_clf_tanh)\n",
    "dnn_clf_tanh.fit(x_train, y_train_int, batch_size = 50, steps = 50000) # 10,000 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_tanh = dnn_clf_tanh.predict(x_test)\n",
    "accuracy_score(y_test_int, y_predict_tanh['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking the Model Parameters to Improve Performance\n",
    "\n",
    "Below we are experimenting with a few different optimizers; since our first dnn classifier saw the best performance, we will be testing the optimizers with it.\n",
    "\n",
    "Since there seems to be some variance with each run of the notebook, we had to create some try-throw-catch statements so that the program would keep running. The first time we did this part, we set the learning rates so there were no errors, when we restarted the notebook and rerun the code, the same parameters gave errors that crashed the program. With the try statements, we will keep the last learning rate that guarantees the dnn to run with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer_1 = tf.train.MomentumOptimizer(learning_rate = .2, momentum = .9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_clf_hidden_optimizer = tf.contrib.learn.DNNClassifier(hidden_units = [300, 150], n_classes = 14, feature_columns = feature_cols, optimizer = optimizer_1)\n",
    "dnn_clf_hidden_optimizer = tf.contrib.learn.SKCompat(dnn_clf_hidden_optimizer)\n",
    "dnn_clf_hidden_optimizer.fit(x_train, y_train_int, batch_size = 50, steps = 50000) # 10,000 (46%); 15,000 (47%), 40,000 (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_hidden_optimizer = dnn_clf_hidden_optimizer.predict(x_test)\n",
    "accuracy_score(y_test_int, y_predict_hidden_optimizer['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer_2 = tf.train.MomentumOptimizer(learning_rate = .5, momentum = .9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_clf_hidden_optimizer_2 = tf.contrib.learn.DNNClassifier(hidden_units = [300, 150], n_classes = 14, feature_columns = feature_cols, optimizer = optimizer_2)\n",
    "dnn_clf_hidden_optimizer_2 = tf.contrib.learn.SKCompat(dnn_clf_hidden_optimizer_2)\n",
    "dnn_clf_hidden_optimizer_2.fit(x_train, y_train_int, batch_size = 50, steps = 50000) # 10,000 (46%); 15,000 (47%), 40,000 (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_hidden_optimizer_2 = dnn_clf_hidden_optimizer_2.predict(x_test)\n",
    "accuracy_score(y_test_int, y_predict_hidden_optimizer_2['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer_3 = tf.train.RMSPropOptimizer(learning_rate = .2, momentum = .9, decay = .9, epsilon = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn_clf_hidden_optimizer_3 = tf.contrib.learn.DNNClassifier(hidden_units = [300, 150], n_classes = 14, feature_columns = feature_cols, optimizer = optimizer_3)\n",
    "dnn_clf_hidden_optimizer_3 = tf.contrib.learn.SKCompat(dnn_clf_hidden_optimizer_3)\n",
    "dnn_clf_hidden_optimizer_3.fit(x_train, y_train_int, batch_size = 50, steps = 50000) # 10,000 (46%); 15,000 (47%), 40,000 (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_hidden_optimizer_3 = dnn_clf_hidden_optimizer_3.predict(x_test)\n",
    "accuracy_score(y_test_int, y_predict_hidden_optimizer_3['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer_4 = tf.train.AdamOptimizer(learning_rate = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf_hidden_optimizer_4 = tf.contrib.learn.DNNClassifier(hidden_units = [300, 150], n_classes = 14, feature_columns = feature_cols, optimizer = optimizer_4)\n",
    "dnn_clf_hidden_optimizer_4 = tf.contrib.learn.SKCompat(dnn_clf_hidden_optimizer_4)\n",
    "dnn_clf_hidden_optimizer_4.fit(x_train, y_train_int, batch_size = 50, steps = 50000) # 10,000 (46%); 15,000 (47%), 40,000 (50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_hidden_optimizer_4 = dnn_clf_hidden_optimizer_4.predict(x_test)\n",
    "accuracy_score(y_test_int, y_predict_hidden_optimizer_4['classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Present Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
